# 
# AQMEN (Data Science for Social Research)
# http://www.aqmen.ac.uk/
#
# 
# Data Wrangling: Organising and Enabling Data
# 
# R and Python Workshop (March 2019)
# 
# A three day hands-on workshop led by Dr Diarmuid McDonnell and Professor Vernon Gayle, University of Edinburgh.
# 
# 
# Topics: 
# 
# This three-day workshop will provide a fast-track introduction for individuals wishing to learn how to work with data suitable 
# for statistical analysis of business problems. Preparing and enabling data (data wrangling) is an essential aspect of undertaking 
# data intensive statistical research. Data wrangling is highly time consuming and can be complex especially when dealing with messy data, 
# which is often encountered in the non-academic world. 
#
# There will be an emphasis on developing accurate, efficient, transparent and reproducible working practices when organising and enabling data.
# 
#
# Rationale: 
# 
# The Industrial Strategy recognises that a major challenge facing UK businesses and industry is how best to utilise big data 
# to improve economic performance and increase productivity. A substantial barrier to exploiting the potential offered by 
# emerging forms of big data is the lack of a suitably trained workforce with appropriate analytical skills. 
#
# Many statistical analysis techniques used in the social sciences are also suitable for working with big data in non-academic settings, 
# however social science graduates often lack experience in applying their skills and knowledge in non-academic research domains.
#
#
# Advice:
#
# The workshop is intended for people who have little prior experience of R.
#
# The aim of the workshop is to equip you with a proficiency in data wrangling using R as rapidly and painlessly as possible.
#
# Therefore, be good to yourself: we explore a multitude of useful data wrangling techniques that often take most of a semester to cover. 
#
# It will NOT be possible to learn everthing in three days (drinks on us if you prove us wrong).
#
# Please be patient. Computers often go wrong.
#
# Please asks the instructors for help.
#
# Feel free to work in pairs during the pratical sessions.
#
# Not all of your questions will be answered but we will help as much as we can.
#
# Good luck.
#
##############################################


##############################################

# Outline of Activities #

# The workshop is based around a series of activities that involve the use of R for organising and enabling administrative data for statistical analysis:

#	1. Getting Started with R: a quick introduction to the R programming language and various data types [ACT001]

#	2. Organising Variables and Measures: how to import and examine data, and handle various types of variables [ACT002]

#	3. Linking and Shaping Data: how to merge and append multiple sources of data [ACT003]

#	4. Dealing with Temporal Data: tools and techniques for working with dates [ACT004]

#	5. Harvesting Web-based Data: how to use Python for accessing information from websites and online databases (APIs) [ACT005]

#	6. Automating Data Wrangling Operations: how to improve the efficiency of your code through loops and functions [ACT006]

#	7. Hackathon: two blocks of time where participants will tackle a data wrangling challenge using administrative data [ACT007]

# If you want to jump to a section: press Ctrl + F and search for the activity code e.g. [ACT001].


##############################################


##############################################
#
#
# We suggest that you make a copy of this file.
#
#
# ##############################################
# # IT IS IMPORTANT THAT YOU READ THIS HANDOUT #
# # AND FOLLOW THE R FILE LINE BY LINE! #
# ##############################################
# 
# The file is sequential. It MUST be run line by line. 
# Many of the commands will NOT run if earlier lines of commands have not been executed.
#
# Anotate your new copy of the file as you work through it with your own notes 
# (use "#" to comment out your notes).
#
#
# Throughout the file there are markers requiring your input:
#	- TASK: a coding task for you to complete (e.g. create new variables)
#	- QUESTION: a question regarding your interpretation of some code or a technique (e.g. what is the above loop doing?)
#	- EXERCISE: a data wrangling challenge for you to complete at the end of an activity
#
# 
# ON WITH THE SHOW!
#
#
##############################################


##############################################


# 0. Software Demonstration #

# 0.1 System Setup #

# Create a R project folder/directory

# Open RStudio and follow these instructions:
# 	- File > New Project
#	- Create project directory

getwd() # tells us the current working directory i.e. workspace
# setwd("C:/Users/mcdonndz-local/Desktop/temp") # set the working directory to a specified directory; however we have no need to
# do this as we have already set up a directory to store all of the components of our R project.

folders = c('data_raw', 'data_clean', 'temp', 'logs') # create a list with folder names
for(f in folders) {
  print(f)
  dir.create(f)
} # take a look at the bottom right-hand panel in RStudio (or the directory on your machine) to check if the folders were created

# Creating and saving files:
data <- file.create("./temp/sampdata.csv")
write.csv(data, "./temp/sampdata_20190321.csv")
# Note the use of "." at the beginning of the file path; this signifies that the current working directory
# should form the first part of the path without needing to be explicitly stated. This is an example of
# using relative file paths and is considered good practice.

# List all files in our working directory:
dir() # list all files in a directory
head(dir(recursive = TRUE)) # list all files in a directory (including its subdirectories); head() restricts the output to the first few results
dir(pattern = "\\.csv$", recursive = TRUE) # find all files that end in ".csv"
# The above command used regular expressions to detect patterns in text.

file.info("./data_raw/sampdata.csv") # displays some basic file information 
# (e.g. size, whether it is a folder, created and modified times)

# That's enough file management for now. There are lots of other tasks we can perform, such as copying, moving, deleting,
# opening, checking if a file exists etc, that we do not cover here: see [http://theautomatic.net/2018/07/11/manipulate-files-r/]

# TASK: move the files from the workshop Dropbox folder to the "data_raw" directory you just created.

# New R functions used in this section:

#	- getwd() - provide working directory location
#	- c() - create a vector of values or objects
#	- print() - display information on the console
#	- file.create() - create a file
#	- write.csv() - export a csv file to a directory
#	- dir.create() - create a directory
#	- dir() - list all files in a directory
#	- head() - list the first few rows (values) of a data frame (object)
#	- file.info() - display information about a file


# 0.2 Installing Packages #

# The real power of using R for data wrangling and analysis comes from the universe of user-written packages that are available.
# A package bundles together code, data, documentation, and tests and provides an easy method to share with others.

# Packages represent both a blessing and a curse: a blessing because it is unlikely you won't be able to find a function you need for your analysis;
# a curse because it adds a bit of administrative burden to your workflow (i.e. find a package, install it, load it, use it). Also,
# help documentation is wildly inconsistent across packages. 

# A package only needs to be installed once, but you will need to load it in every time you launch an R session.

my_packages <- c("tidyverse", "car", "haven", "expss") # create a list of desired packages

install.packages(my_packages, repos = "http://cran.rstudio.com") # install packages from the CRAN repository

installed.packages() # check which packages have been installed

.libPaths() # check which folder the packages are downloaded to


# 0.3 Loading Packages #

library(tidyverse) # load in the "tidyverse" package of data wrangling functions
?tidyverse
vignette("tidyverse")

# TASK: load in the "car", "haven", and "expss" packages.

# A final note about packages: you'll see mention of performing functions or tasks using base R. This means drawing on the functions that come
# as standard with your version of R. install.packages() and write.csv() are examples of base R functions.

# For the purposes of data wrangling (and most other data analysis tasks, frankly), we will not use base R functions; the reasons will become clear
# as we progress but it is worth noting that there is more than one way to skin a cat.

# New R functions used in this section:

#	- install.packages() - install R packages
#	- installed.packages() - display installed R packages
#	- .libPaths() - display directory of installed R packages
#	- library() - load in R package
#	- vignette() - view examples of how to use the function/package (not available for all functions/packages)

##############################################


##############################################


# 1. Getting Started with R [ACT001] #

# R is a programming language. It has rules, packages, syntax, complexities, idiosyncracies...
# It is not particularly easy to learn, nevermind master. It can seem as if you have to learn everything in order
# to do anything!

# Persevere: like any language, once you grasp the building blocks you will begin to feel comfortable. All of the fancy models, code and graphs
# that make it into journal articles, textbooks, presentations etc are just extensions and flourishes added on top of the basic functions and rules.

# The important thing is to expect failure and react accordingly (just like an astronaut).


# 1.1 Comments #

# This is a comment
## This is also a comment
###### ...you get the idea

# Comments are an important means of documenting your workflow and ensuring others (including future-you) can reproduce your work.

# In R studio, you can create multiline comments by highlighting the text and pressing Ctr + Shift + C. For example:

This should be a comment and not code.
Excuse me, did you hear me?
HELLLOOOO!
How rude...


# 1.2 Writing Code #

print("Hello World!") # display a message to the console

# To execute (i.e. run) the above code, highlight it and press Ctrl + Enter, or the Run icon in the top-left panel in RStudio.

# TASK: print your own personalised message to the console.


# 1.3 Data Types #

# Variables are known as 'objects' in R and can store a wide variety of data types:
# - numeric
# - string
# - boolean etc

# Each data type can have different classes i.e. numeric has integer and double (e.g. decimal).

# We assign a value to an object using the "<-" operator. We can also use "=" but this is best avoided as the equals sign has another use
# and "<-" is considered standard practice in R.

# 1.3.1 Numeric #

x <- 5 # Integer
y <- 5.5 # Double or Float

# Notice how RStudio doesn't print the value of x or y. To evaluate the assigment you need to call the object:
x
y

# Assign and evaluate in a single command:
(x <- 5)
# Our advice is to keep assignment and evaluation commands separate (those parentheses can add confusion and lead to errors) but the choice is yours...

print(x + y) # print ensures the result is displayed in the console or output window

# We can compare objects using a set of comparison operators:
x == y
x < y
x > y
x != y
x >= y
x <= y
# TASK: document what each of the comparison operators does.

a <- c(1, 4, 9, 12)
b <- c(4, 4, 9, 13)
a == b # compares each number in the vector to its corresponding number in the other vector

# Note that logical values TRUE and FALSE equate to 1 and 0 respectively, allowing us to perform arithmetic operations using these results:
sum(a == b) # 2 instances where the elements of the vector are equal

# To test if two objects are exactly equal:
identical(x, y)

print(typeof(x))
print(typeof(y)) # R stores numbers as a double by default; we need to be specific when assigning the object's value(s):

rm("x") # remove the objects from R
rm("y") # check the environment pane in the top-right hand corner of RStudio to see what objects remain in the global environment

x <- 5L # rather counterintuitively given that it's a letter, the "L" suffix ensures a number is stored as an integer
y <- 5.5
print(typeof(x)) # Now it is stored explicitly as an integer; in practice you often do not need to worry about this
print(typeof(y)) 

# Another approach is to convert an existing object:
int_var <- 20
int_var <- as.integer(int_var)
print(typeof(int_var))

# Vectors

# Vectors provide a means of structuring data types as a list

vec <- 1:10
print(vec) # creates a vector from 1 to 10; a vector is a list of values stored in a single object
vec[1] # return the first element in vec
vec[1:5] # return the first five elements in vec
vec[-2] # return the values of the vector, excluding the second element
vec[-1:-5]
# TASK: describe the results of "vec[-1:-5]".

# The above commands are known as 'slicing' i.e. accessing a particular element(s) in a vector.

# You can also store objects as a vector:
ovec <- c(x, y) # combine the objects "x" and "y" in a vector called "ovec"
ovec

# You can count the number of elements in a vector:
length(vec)

# You can also drop elements:
vec <- vec[-2] # note that we overwrite the existing object; we could just as easily assign a new object to preserve the original
vec2 <- vec[-(4:6)] # drop 4-6 from the vector

# We can perform calculations with vectors:
a <- c(1, 2, 3, 4, 5)
b <- c(6, 7, 8, 9, 10)
c <- c(1, 2, 3)

a + b # adds each element of the vectors together in order (i.e. 1 + 6, 2 + 7 etc)
# This is known as vectorization and is a very useful property of R.

a + c # generates a warning that the vectors are not multiples of each other (i.e. one has 5 elements, the other 4)
# When vectors are of unequal length, the shorter vector is "recycled" i.e. goes back to the start.

# Generate a sequence of numbers

sequence <- seq(from = 1, to = 100, by = 5)
print(sequence) 
# TASK: describe what the seq() function is doing above.
# TASK: create a sequence of numbers that starts at 55, ends at 7000, and increases by 55 each time.

# Create a sequence based on repeating or replicating the numbers
repetition <- rep(1:10, each = 10)
print(repetition)

# Generating sequences of random numbers

# This is a useful function for performing simulations or generating data for testing ideas and techniques

# Generate 100 random numbers between 0 and 25 from a uniform distribution i.e. each number has an equal probability of being selected
runif(100, min = 0, max = 25)

# Generate 100 random numbers between 0 and 25 (with replacement)
sample(0:25, 100, replace = TRUE)

# Generate 100 random numbers between 0 and 25 (without replacement)
sample(0:25, 100, replace = FALSE)

# QUESTION: why can we not sample 100 numbers from this range without replacement?

# Generate 1000 random numbers from a normal distribution with given mean and standard deviation
normdist <- rnorm(1000, mean = 0, sd = 1)
hist(normdist) # approximately normal 
print(summary(normdist))

# Generate CDF probabilities for value(s) in vector q
pnorm(0.5, mean = 0, sd = 1)

# Generate quantile for probabilities in vector p
qnorm(0.5, mean = 0, sd = 1)

# Generate density function probabilites for value(s) in vector x
dnorm(0.5, mean = 0, sd = 1)

# Generate a vector of length n displaying the number of successes from a trial of size = 100 with a probabilty of success = 0.5
rbinom(10, size = 100, prob = 0.5)
# QUESTION: how many successes were there in 10 trials, each with a sample size of 100?

# Generate a vector of length n displaying the random number of events occuring when lambda (mean count) equals 4.
rpois(20, lambda = 4)
# TASK: vary the number of expected events and interpret the results.

# We can reproduce random numbers by setting the seed:
set.seed(1) # name the random sample "1"
rsamp1 <- rnorm(n = 10, mean = 0, sd = 1)
set.seed(1)
rsamp2 <- rnorm(n = 10, mean = 0, sd = 1)
print(rsamp1)
print(rsamp2) # produces the same values in each random sample

# Rounding numbers

x <- c(1, 1.35, 1.7, 2.05, 2.4, 2.75, 3.1, 3.45, 3.8, 4.15, 4.5, 4.85, 5.2, 5.55, 5.9)
print(x)

# Round to the nearest integer
round(x) # note how the original object is not altered - run the command "print(x)" to check

# Round up
ceiling(x)

# Round down
floor(x)

# Round to a specified decimal
round(x, digits = 1)


# 1.3.2 Strings #

# Strings (text) are stored in the character class in R.

a <- "learning to create" # create string a
b <- "character strings" # create string b
paste(a, b) # combine the strings

# Paste character and number strings (converts numbers to character class)
paste("The life of", pi)

# Paste multiple strings
paste("I", "love", "R")

# Paste multiple strings with a separating character
paste("I", "love", "R", sep = "-")

# Converting to strings

a <- "The life of"
b <- pi
is.character(b) # check if b is a string
c <- as.character(b)
is.character(c)

# Printing strings

print(a)
print(a, quote = FALSE) # easier to use the command "noquote(a)"
noquote(a)

cat(a)
cat(a, "Riley") # the cat function is useful for printing multiple objects in a readable format
cat(letters)

x <- "Today I am learning how to print strings."
y <- "Tomorrow I plan to learn about something else."
z <- "The day after that I will take a break and drink a beer."
cat(x, y, z, fill = 1) # the fill option specifies line width

# Substituting strings and numbers

x <- "The R package is great"
sprintf("You know what? %s", x) # think of "%s" as a placeholder for a string stored in an object
TASK: call the help documentation for the "sprintf()" function.

y <- 0
sprintf("You know what? I had %d beers last night", y)
sprintf("Here are some digits from Pi: %f", pi) # "%f" is a placeholder for a number stored in an object

# Counting string elements and characters

length("How many elements are in this string?")
length(c("How", "many", "elements", "are", "in", "this", "string?"))

nchar("How many characters are in this string?")
nchar(c("How", "many", "characters", "are", "in", "this", "string?"))
# Counting elements and characters becomes very useful when constructing loops.

# Special characters

string2 <- 'If I want to include a "quote" inside a string, I use single quotes'
string3 <- "\""
string4 <- "\'" # if we want to include a single or double quote in our string we use the backslash (\) to escape the character
TASK: include a backslash in a string.

x <- c("\"", "\\")
x
writeLines(x) # beware that the printed representation of a string is different from the contents of the string itself
# Special characters are very useful in R but they can throw a spanner in the works; we'll deal more with them later in the workshop.

# String manipulation with stringr

# We can perform a lot of the core string manipulation tasks (e.g. removing whitespace, converting to lowercase etc)
# using base R functions. However we will use a package that simplifies the syntax: stringr

help("stringr")

# All functions in stringr start with str_ and take a vector of strings as the first argument:

x <- "Hello, this is a run-of-the-mill string."
str_length(x)
str_c(x, " ", "Not very interesting at all.") # combine strings
str_sub(x, 1, 10) 
# QUESTION: what is the str_sub function doing to the string?

y <- c("Hello", "This", "is a bog standard", "string")
str_subset(y, "[aeiou]") # returns strings matching the pattern i.e. contain a vowel
str_subset(y, "[qrstuvwxyz]")

# Change text to upper, lower or title case
uc <- "DOWN WITH THAT SORT OF THING"
lc <- "careful now"
str_to_upper(lc)
str_to_lower(uc)
str_to_title(uc) 
# QUESTION: what tv show are those strings referencing an iconic moment from?

# String matching

str_detect(y, "[aeiou]") # tells you if there’s any match to the pattern
str_count(y, "[aeiou]") # counts how many vowels are in each string
str_locate(y, "[aeiou]") # gives the position of the first match
str_extract(y, "[aeiou]") # extracts the text of the first match
str_match(y, "(.)[aeiou](.)") # extract the characters on either side of the vowel
str_replace(y, "[aeiou]", "?") # replace first match with a specified character
str_split(x, "") # split a string into individual characters based on a specified separator
str_dup(x, times = 10) # duplicates the string n times

# Removing leading and trailing whitespace

text <- c("Text ", " with", " whitespace ", " on", "both ", " sides ")
print(text)

# Remove whitespaces on both sides
str_trim(text, side = "both") # other options include "right" and "left"
str_pad("beer", width = 10, side = "left") # add whitespace on the left of the string

# Set operations for strings

set_1 <- c("lagunitas", "bells", "dogfish", "summit", "odell")
set_2 <- c("sierra", "bells", "harpoon", "lagunitas", "founders")
union(set_1, set_2) # list all individual elements from the sets
intersect(set_1, set_2) # list all common elements from the sets
setdiff(set_1, set_2) # returns elements in set_1 not in set_2; swap order of sets

# To test if two vectors contain the same elements regardless of order use setequal():

set_3 <- c("woody", "buzz", "rex")
set_4 <- c("woody", "andy", "buzz")
set_5 <- c("andy", "buzz", "woody")
setequal(set_3, set_4)

set_6 <- c("woody", "andy", "buzz")
identical(set_4, set_6) # check if sets are exactly equal (elements and order)

# Identifying if element is in string
good <- "andy"
bad <- "sid"
is.element(good, set_5) # is the word "andy" in set_5?
good %in% set_5 # same as above
# TASK: see if the word "sid" is in set_3.

# Sort a string
sort(set_5)
sort(set_5, decreasing = TRUE)

# That's enough of strings. The information they contain can be of considerable interest for quantitative data analyses and it is worth
# beginning familiar with their storage and manipulation in advance of more sophisticated analyses (e.g. Natural Language Processing).


# 1.3.3 Categorical Variables #

# Known as factor variables in R (and other packages like Stata etc), 
# they group observations into exhaustive and mutually exclusive categories.
# We'll use the forcats package to work with categorical variables in R:
library(forcats) # not part of the core tidyverse package so we need to load it in separately

# Create a factor

x <- c("male", "female", "female", "male", "female") # list of observations for biological sex
sex <- factor(x)
sex
# QUESTION: how many observations and levels are there for this factor variable?

class(sex) # confirm it is a factor variable
unclass(sex) # show the underlying values of this variable: female = 1, male = 2 (numeric values are attached to categories alphabetically by default);

# We can change the order in which numbers are attached to categories by specifying the "levels" option:
sex2 <- factor(x, levels = c("male", "female"))
levels(sex2)  # display the levels (i.e. categories) of this variable
summary(sex2) # summarise the variable i.e. frequency count

# We can convert an existing list of strings to a factor variable:
group <- c("Group1", "Group2", "Group2", "Group1", "Group1")
group2 <- factor(group)
levels(group2)

# Instead of numbering categories alphabetically, we can do so according to when a factor first appears:
las <- c("Glasgow", "Edinburgh", "Aberdeen", "Glasgow", "Orkney", "Edinburgh")
cat_las <- factor(las, unique(las))
attributes(cat_las)
unclass(cat_las)

# Ordering factor variables
ses <- c("low", "middle", "low", "low", "low", "low", "middle", "low", "middle",
         "middle", "middle", "middle", "middle", "high", "high", "low", "middle",
         "middle", "low", "high")
ses <- factor(ses, levels = c("low", "middle", "high"), ordered = TRUE)
print(ses) # categories are ordered from "low" to "high"
factor(ses, levels = rev(levels(ses))) # you can also reverse the order of levels if desired

# Recoding categorical variables

# The "plyr" package is useful for this task:
new_ses <- plyr::revalue(ses, c("low" = "small", "middle" = "medium", "high" = "large"))
print(new_ses)
levels(new_ses)
# Note that using the :: notation allows you to access the revalue() function without having to fully load in the plyr package.
# There are other ways of recoding categorical variables, none of which (in my opinion) are as easy as Stata's approach.

# Dropping categories with no observations

ses_2 <- ses[ses != "middle"] # create a new variable where ses does not equal the value "middle"
summary(ses_2)
droplevels(ses_2)


# 1.4 Saving Files #

# We can save the current workspace (i.e. all of the objects and/or functions we have created):
save(file="./temp/dw_workspace.RData") # RData is the file extension for a workspace
load("c:/temp/dw_workspace.RData") # load in a workspace

# However, we should have no need to save the workspace for a data wrangling/analysis piece of work;
# we simply need our syntax file (R script) and raw data and we should be able to reproduce our project.

# We can save our R script by pressing Ctrl + S (on Windows) or going to the File menu and selecting Save.


# 1.5 Getting Help #

help.start() # provides general help links
help.search("regression") # searches the help system for documentation matching a given character string

help("strtrim") # finds help documentation on the "strtrim" function
?strtrim # another way of searching for help
example("strtrim") # display example code using this function
# In RStudio, you can also highlight the function in your code and press F1.

help(mean) # let's dissect the help material for the mean() function
vignette("dplyr") # some of the better-documented packages provide detailed examples of how to use its functions

# Can't quite remember the name of an object or function?
apropos("sum") # returns all objects in the global environment that contain the text "sum"

# If you need help from the web then [insert chosen search engine] is your friend.
# Likewise, the Stackoverflow website is an excellent source of help on many programming languages and data wrangling/analysis problems.
# If you experience an error message then follow the above advice by searching for the exact message you receive i.e. don't paraphrase
# your issue.


# 1.6 Keyboard Shortcuts #

# To execute R code: highlight the syntax and press Ctrl + Enter.

# To execute the entire R script (i.e. all of the code in one): press Ctrl + Shift + S.

# To insert the assignment operator (i.e '<-'): press Alt + minus key (-).

# To autocomplete your syntax: start typing the name of an object/function and press TAB.

# To insert the pipe operator (%>%): press Ctrl + Shift + M.


# 1.7 Troubleshooting #

# If you run some code and nothing happens, check the console (bottom-left pane in RStudio): if you see a plus sign (+) then
# R thinks you haven't finished writing the command and expects more code. If this happens then press the ESC key to cancel the
# command.

# R is case sensitive e.g. 'View(data)' displays the data set in a window (similar to the 'browse' function in Stata);
# 'view(data)' does nothing.


# 1.8 Debugging #

# This is the computer science term for dealing with code issues. R likes to tell you when something is not quite right,
and not always in an intelligble manner. As progress with this workshop, you are likely to encounter the following results:
#	- message: R is communicating a diagnostic message relating to your code; your commands still execute.
#	- warning: R is highlighting an error/issue with your code, your commands still execute but the warnings need addressing.
#	- error: R is telling you there has been a fatal error with your code; your commands do not execute.

# Let's look at a simple example:
log(-1) # take the natural log of -1
# This warning tells you that the output is missing i.e. there is no natural log of a negative number.
warning() # displays the warnings associated with the most recently executed block of code

# You'll encounter plenty of messages, warnings and errors over the course of this workshop. For now, here is some general
# advice from Peng (2015) regarding what questions to ask when debugging:
#	- What was your input? How did you call the function?
#	- What were you expecting? Output, messages, other results?
#	- What did you get?
#	- How does what you get differ from what you were expecting?
#	- Were your expectations correct in the first place?
#	- Can you reproduce the problem (exactly)?


# 1.9 Environment Objects #

# We can remove some of the objects we've created (i.e. delete variables):
ls() # list existing objects

a <- "I am a useless object"
rm("a") # delete the object "a"

exists("x") # check if the object "x" exists
rm(c("x", "y")) # you can remove multiple objects by using the "c()" function

history(Inf) # displays all of the commands executed in this R session


# 1.10 Workspace Options #

help(options)
options() # wide range of options for displaying results etc
options(digits=3) # change a specific option (i.e. number of digits to print on output)
options(max.print = 9999) # set maxmimum number of rows to print to the console as 9999


# 1.11 Data Wrangling Examples #

# Congratulations on getting through the technical (boring) bit of the first activity. To whet your appetite, here are
# some examples of the techniques you will learn over the course of the workshop.

# Importing data

# Importing as a csv file

auto <- read_csv("./data_raw/auto.csv")
str(auto)
auto

# Exporting data

# Exporting as a csv file

write_csv(auto, "./data_clean/auto2.csv")
# write_csv() has two useful properties:
#	- it encodes strings in UTF-8
#	- it encodes dates as ISO8601
# Both of these mean that the resulting file can be easily parsed by other languages/programs.

# Exporting as a xlsx file

library(readxl)
write_xlsx(auto, file = "./data_clean/auto2.xlsx")

# Exporting as an R file

saveRDS(auto, "./data_clean/auto2.rds") # save the auto data set as an R data file
readRDS("./data_clean/auto2.rds") # load in the R data file

# Create a data set

# Data sets are known as "data frames" in R/Python and are usually imported into R various functions (e.g. read_csv()).
# However we can also create our own data frames using data.frame(); this comes in handy
# not only for teaching purposes but also for simulation and exploration purposes.

df <- data.frame(col1 = 1:3,
                 col2 = c("this", "is", "text"),
                 col3 = c(TRUE, FALSE, TRUE),
                 col4 = c(2.5, 4.2, pi))
# TASK: describe the structure of this data frame. Hint: use "str(df)".

nrow(df) # return the number of rows in the data set
ncol(df) # return the number of columns in the data set

# New R functions used in this section:

#	- length() - return the length of an object
#	- rm() - remove an object from the workspace environment
#	- read_csv() - import csv files into R
#	- export_csv() - export data as csv files from R
#	- read_excel() - import xls/xlsx files into R
#	- write_xlsx() -  export data as xls/xlsx files from R
#	- options() - change default R settings
#	- ls() - list objects in workspace environment
#	- str() - examine the structure of an object
#	- factor() - create a categorical variable
#	- class() - return an object's class


##############################################


##############################################


# 2. Organising Variables and Measures [ACT002] #

# In this section we focus on importing data, and examining and manipulating variables.
# We are going to work with large-scale, messy and longitudinal administrative data on U.K. charities.
# A data dictionary can be found on the Charity Commission's website [http://data.charitycommission.gov.uk/data-definition.aspx]

# 2.1 Importing Data Sets

# This is the latest copy of the Charity Register i.e. list of all registered charities in England and Wales:
char_reg <- read_csv("./data_raw/extract_main_charity.csv")
# Ignore the warnings for now; we'll address those later.

View(char_reg) # browse the data set
head(char_reg) # view the first few rows of the data set
tail(char_reg) # view the last few rows of the data set
names(char_reg) # list of variable names
str(char_reg) # examine the structure of the data set
attributes(char_reg) # list the attributes (i.e. metadata) of the data set
ncol(char_reg) # number of columns
nrow(char_reg) # number of rows
order(char_reg$regno) # sort the data set by ascending order of charity number

# We can also add a label or comment to the dataset:
comment(char_reg) <- "This dataset contains observations for registered charities in England & Wales (February 2019)"
attributes(char_reg)
# TASK: try adding a second comment to the dataset; what happens?

# TASK: import the "extract_remove_ref.csv" file and describe the structure and contents of the data set, and label the data set.

# 2.2 Examining Variables

str(char_reg) # list the variables and their data type
class(char_reg$income) # list the class of the income variable - it is numeric
# Note the use of $col_name suffix to access variables in the data set i.e. data$variable.

(var_lab(char_reg$income) <- "Latest annual gross income of a charity") # add a variable label using var_lab() function from "expss" package
unlab(char_reg$income) # drop the variable label

# TASK: add labels for all of the variables; refer to the data dictionary to understand the meaning of each column.

head(char_reg$fyend) # day and month of a charity's financial year end
class(char_reg$fyend) # stored as character data type - we want this to be a factor (categorical) variable
(char_reg$fyend_cat <- as.factor(char_reg$fyend)) # create a new variable
# QUESTION: what is the effect of enclosing the command in parentheses?

levels(char_reg$fyend_cat) # hmmm, probably too many categories to be useful; let's drop this new variable:
char_reg <- within(char_reg, rm(fyend_cat)) # remove the variable from the data set using the rm() function
names(char_reg) # check if the variable was deleted

# TASK: call the help documentation for the within() function and write a note describing its use.

median(char_reg$income, na.rm = TRUE) # calculate median income, ignoring missing values
summary(char_reg$income) # summary statistics for annual gross income; however, too difficult to read (in scientific notation)
options(scipen=999) # change formatting display of numbers (no scientific notation)

# TASK: run the summary() function once more. What is the mean income in the charity sector?

char_reg$linc <- log(char_reg$income) # create a new variable that is a log transformation of annual gross income;
# This is often a good idea as charity income is usually heavily positively skewed.

# TASK: generate a variable for income squared.

# Subsettting data

char_reg_sub <- subset(char_reg, select = c(regno, income, fyend)) # keep certain variables
str(char_reg_sub)

char_reg_fil <- subset(char_reg, income >= 500000) # keep certain observations
summary(char_reg_fil$income)

# TASK: drop the "char_reg_sub" and "char_reg_fil" objects from the workspace.

# Dealing with duplicates

distinct(char_reg) # drops duplicate rows from the data set

char_reg %>%
  distinct(regno) # drop rows where there are duplicates of charity number; what is this new symbol %>%?

# Piping

# No, not the Scottish kind... Think of piping as a process for building summary tables. It takes
# data as an input, transfers it into some functions, and converts it into some results (like a pipeline). 
# In the words of Healy (2019):
# "A pipeline is typically a series of operations that do one or more of four things:
#   1. Group the data into the nested structure we want for our summary, such as “Religion by Region” or “Authors by Publications by Year”.
#   2. Filter or select pieces of the data by row, column, or both. This gets us the piece of the table we want to work on.
#   3. Mutate the data by creating new variables at the current level of grouping. This adds new columns to the table without aggregating it.
#   4. Summarize or aggregate the grouped data. This creates new variables at a higher level of grouping. For example we might calculate means with mean() or counts with n(). This results in a smaller, summary table, which we might do more things on if we want."
# The pipe operator = %>% (Ctrl + Shift + M)

# It's a slightly abstract way of performing tasks, so let's dig into some more examples:

char_reg %>%
  distinct(income) # we can see that the results of this command greatly reduces the size of the data set (79,919 rows)

char_reg2 <- char_reg %>%
  distinct(income) # store the results of the distinct command in a new object (char_reg2)

# TASK: explore distinct observations for some of the other variables in the data set.

# Dealing with missing data

# Think of missing values as being of two types:
#	1. Explicitly missing
#	2. Implicitly missing
# The former is the presence of an absence (i.e. missing values are flagged as NA or 99), the latter the absence of a presence (i.e.
# we do not possess an observation for that person for whatever reason).

# In R, missing values are often represented by NA or some other user-specified value (e.g. 99).
# Missing values are problematic as any the result of any operation involving them will also be missing (unknown). For example:
x <- NA
y <- NA
x + 10
y - 500
NA == NA

is.na(char_reg$income) # identify which values of the variable are missing
which(is.na(char_reg$income)) # identify where in the variable the missing values are (i.e. the row number)
sum(is.na(char_reg$income)) # count the number of missing values
# Note how we wrapped the is.na() function inside the sum() function. This is a powerful feature of R and should be
# kept in mind as you progress.

# TASK: count the number of missing values for charity number, income date and email.

# We can also use the complete.cases() function to identify and exclude rows with missing values:
complete.cases(char_reg) # rows three and four have missing values for some/all of their variables
char_reg[!complete.cases(char_reg), ] # list missing rows
# Note the use of "!" in the above command: this is a logical negation e.g. NOT EQUALS. In the case above, the "!"
# is returning the opposite of complete.cases().

char_reg_nomiss <- na.omit(char_reg) # list non-missing rows
nrow(char_reg_nomiss)

# QUESTION: how many observations in the data set have no missing values for all of the variables?
# TASK: create a new data set that only contains observations for which we have non-missing values for income.

# We can exclude missing values from calculations and functions as follows:
mean(char_reg$income)
# QUESTION: why is the mean() function not returning a numeric value?

mean(char_reg$income, na.rm = TRUE)

# We can recode missing values quite easily:
char_reg$income[char_reg$income == NA] <- -9
char_reg[!complete.cases(char_reg$income), ]

# Note the use of "[]"; this is the subsetting operator in R. The above command translates to: "change values of income
# to -9 for observations where income equals NA."









# 4. Dealing with Temporal Data #

# Dealing with Dates #

# Dates are represented by the Date class and times are represented by the POSIXct or the POSIXlt class. 
# Dates are stored internally as the number of days since 1970-01-01 while times are stored internally as the number of seconds 
# since 1970-01-01.

# System properties

Sys.timezone() 
Sys.Date()
Sys.time()

# Converting strings to dates
# This is an important process as dates are often imported in R and other software packages
# as strings.
x <- "2015-07-01" # string in YYYY-MM-DD format
z <- as.Date(x)
class(z)
class(x)

y <- "07/01/2015" # string in MM/DD/YYYY format
z <- as.Date(y, format = "%m/%d/%Y") # specify format of date
class(z)
class(y)
# Now that the information is in date format we can perform calculations involving units of time

# While we've used the base R functions to work with dates so far, we are better off
# using a dedicated package for working with this data type: "lubridate"
# Most of the material in this section is derived from: https://data.library.virginia.edu/working-with-dates-and-time-in-r-using-the-lubridate-package/

library(lubridate)
?lubridate # comes as part of the "tidyverse" collection of data science packages

begin <- c("May 11, 1996", "September 12, 2001", "July 1, 1988")
end <- c("7/8/97","10/23/02","1/4/91") # sample observations for the start and end date of a process
class(begin)
class(end)

(begin <- mdy(begin)) # convert to date, replacing the original object
(end <- mdy(end)) # convert to date, replacing the original object
class(begin)
class(end)

# The “Date” class means dates are stored as the number of days since January 1, 1970, 
# with negative values for earlier dates. 
# We can use the as.numeric function to view the raw values:
as.numeric(begin)
as.numeric(end)

# We can also use "lubridate" for handling dates with hours, minutes and seconds:
begin <- c("May 11, 1996 12:05", "September 12, 2001 1:00", "July 1, 1988 3:32")
begin <- mdy_hm(begin)
class(begin) # the dates are now interpeted as the number of seconds since January 1, 1970
print(begin)

"lubridate provides three classes, or three different ways, to distinguish between different types of time spans.
1. Duration
2. Interval
3. Period

Understanding these classes will help you get the most out of lubridate.

The most simple is Duration. This is simply a span of time measured in seconds. There is no start date.

An Interval is also measured in seconds but has an associated start date. An Interval measures elapsed seconds between two specific points in time.

A Period records a time span in units larger than seconds, such as years or months. 
Unlike seconds, years and months differ in time. June has 30 days while July has 31 days. 
February has 28 days except for leap years when it has 29 days. 
With the Period class, we can add 1 month to February 1 and get March 1. 
It allows us to perform calculations in calendar or clock time as opposed to absolute number of seconds." (Ford, 2017: https://data.library.virginia.edu/working-with-dates-and-time-in-r-using-the-lubridate-package/)

start <- mdy_hm("2-11-2019 5:21")
end <- mdy_hm("2-12-2019 5:21")
# Calculate interval between these dates
time_interval <- start %--% end # the "%--%" operator calculates the interval between two date objects
print(time_interval)
print(str(time_interval)) # examine the structure of the object

time_duration <- as.duration(time_interval)
print(time_duration) # displays number of seconds in the interval

time_period <- as.period(time_interval)
print(time_period)
str(time_period)

# We can also use lubridate to extract certain properties of a date object e.g. the year or month
x <- c("2015-07-01", "2015-08-01", "2015-09-01")
year(x)
month(x)
day(x) # day of the month
wday(x) # day of the week
wday(x, label = TRUE, abbr = FALSE) # day of the week with label

# We can alter the values in a date object:
# update(x, year = c(2013, 2014, 2015), month = 9)
# print(x) # NOT WORKING

# Lubridate makes it easy to create sequences of dates:
sampyears <- seq(ymd("2010-1-1"), ymd("2015-1-1"), by = "years")
print(sampyears)
sampyears <- seq(ymd("2010-1-1"), ymd("2015-1-1"), by = "months")
print(sampyears)

# Calculations with dates
# "Since R stores date and time objects as numbers, this allows you to perform various calculations
# such as logical comparisons, addition, subtraction, and working with durations." (Boehmke, 2016: 71)
x <- Sys.Date()
x

y <- as.Date("2015-09-11")
x > y
x - y
# TASK: describe the results produced by the above block of code.

y + days(4)
OlsonNames() # lists time zones


# Importing Data #

# In this section we focus on importing tabular data stored in text, Excel and R files.
# R provides a number of base functions for this process (e.g. read.table()), however we will use
# a package designed specifically for this task: readr. It is faster and its default settings remove the need to specify options.

library(readr)
?readr
# readr is a great wee package and it is worth learning more about how it parses a file. See https://r4ds.had.co.nz/data-import.html
# for a thorough overview of how this package interprets a file and its contents.

# Reading data from csv files

auto <- read_csv("C:/Users/mcdonndz-local/Desktop/github/aqmen_wp3/data/auto.csv")
str(auto)
auto
# read_csv() also has a number of useful options: col_types allows you to specify the data type or class of each column;
# n_max allows you to read in n rows from the data file.

auto <- read_csv("C:/Users/mcdonndz-local/Desktop/github/aqmen_wp3/data/auto.csv", n_max=25)
str(auto)
auto # only read in first 25 rows of the data

auto <- read_csv("C:/Users/mcdonndz-local/Desktop/github/aqmen_wp3/data/auto.csv", skip=1) # skip the first line in the file; useful
# when you don't want to import the column headings or if the first line contains text instead of observations.

auto <- read_csv("C:/Users/mcdonndz-local/Desktop/github/aqmen_wp3/data/auto.csv", col_names = FALSE)
# By default read_csv() treats the first line as the column headings; we can override this with 'col_names = FALSE'.

auto <- read_csv("C:/Users/mcdonndz-local/Desktop/github/aqmen_wp3/data/auto.csv", na = ".")
# Tells read_csv() to treat instances of "." as missing.

# Reading data from txt files

auto <- read_delim("C:/Users/mcdonndz-local/Desktop/github/aqmen_wp3/data/auto.txt", delim=",", col_names = TRUE)
str(auto)
auto
# Using read_delim() is more appropriate when the delimiter is not a comma (e.g. a hyphen, newline, tilde).

# Reading data from Excel files

library("readxl")
?readxl
auto <- read_excel("C:/Users/mcdonndz-local/Desktop/github/aqmen_wp3/data/auto.xlsx")
str(auto)
auto
# read_excel() also has a number of useful options: sheet allows you to specify which spreadsheet to read in;
# col_names allows you to read in specific columns; skip enables you to skip a particular row in the data; 
# na tells read_excel() how missing values are represented in the data (e.g. na = "999")

auto <- read_excel("C:/Users/mcdonndz-local/Desktop/github/aqmen_wp3/data/auto.xlsx", sheet = "Sheet1")
str(auto)
auto
# QUESTION: why is the data frame empty?

# Reading data from R files

# auto <- load(file = "C:/Users/mcdonndz-local/Desktop/github/aqmen_wp3/data/auto.rds") 
# str(auto)
# auto
# # NOT WORKING #

# Reading data from dta files (Stata data sets)

library(haven)
auto <- read_dta("C:/Users/mcdonndz-local/Desktop/github/aqmen_wp3/data/auto.dta")
# You can use haven to import .sav and .sas files also.

# Dealing with problems

# While readr does a good job at guessing the correct data type of the variables, it is not infallible. This is because it only
# examines the first 1000 values of a variable in order to make its guess. Issues can also arise when a variable contains lots
# of missing values. Let's highlight some of these issues using a data set provided by the readr package:
challenge <- read_csv(readr_example("challenge.csv"))
problems(challenge)
# TASK: describe the issue(s) readr is having importing this data set.

# We can specfiy the variable types ourselves using the col_types() function:
challenge <- read_csv(readr_example("challenge.csv"),
	col_types = cols(x = col_double(), y = col_date())) # specify the correct column types




# Examining Data #

library(tidyverse)
installed.packages()


# Dealing with text data #

# Regular Expressions 

# A regular expression (regex/regexp) is a sequence of characters that form a search pattern
# and is an extremely useful function for pattern matching with text. Think of it as a grammer for detecting patterns in text.
# There are base functions in R but we can use the "stringr" package for regex purposes also.
library(stringr)
library(tidyverse)

sdata <- state.name # use the built-in dataset "state.name"
print(sdata)
str(sdata)

# Basic matching

str_view(sdata, "or") # find text containing the string "or"
str_view(sdata, ".r.") # find any three-letter string with "r" in the middle
# Notice how a "." (period) is a special character in regular expressions.
# QUESTION: how would you search for a "." using regular expressions?

str_view(sdata, "\\.") # search for a "." in the text
# A backslash is also a special character in regular expressions, hence the need for two of them in the above code.
# If you want to find a backslash, you need four backslashes in your search term!
# An alternative is to enclose a special character in []:
str_view("My text.", "[.]") # find the "." in the string "My text."
# This approach doesn't work for the following special characters: ] \ ^ and -

# Anchoring

str_view(sdata, "^N") # finds text where "N" is at the beginning of a string
str_view(sdata, "k$") # finds text where "k" is at the end of a string

# Character classes and alternatives

# What do we do when we want to match more than one character i.e. find "this" or "that" or "those" in a string?
# Thankfully, regular expressions provide a simple means of doing so (Grolemund & Wickham, 2017):
# - \d: matches any digit.
# - \s: matches any whitespace (e.g. space, tab, newline).
# - [abc]: matches a, b, or c.
# - [^abc]: matches anything except a, b, or c.

str_view(c("grey", "gray"), "gr(e|a)y") # find "grey" or "gray" i.e. find "gr" followed by an "e" or an "a" and then a "y"
str_view("I have 0 dogs and 1 wife in my life", "\\d") # find any digit in the string; it only highlights the first instance however
str_extract_all("I have 0 dogs and 1 wife in my life", "\\d") # returns all digits to the console

str_view(sdata, "[wxyz]") # finds text containing any of these consonants

# Repetition

# Now we focus on how many times a pattern matches:
x <- "1888 is the longest year in Roman numerals: MDCCCLXXXVIII"
str_view(x, "CC?") # matches zero or one times
str_view(x, "CC+") # matches one or more times
str_view(x, 'C[LX]+')
# TASK: describe the results of the above command.

# You can also specify the number of matches precisely (Grolemund & Wickham, 2017):
# - {n}: exactly n
# - {n,}: n or more
# - {,m}: at most m
# - {n,m}: between n and m

# TASK: using the object x, find two instances of the letter "C", three instances of the letter "X", 
# and one instance of the letter "I".

# Detect matches

x <- c("apple", "banana", "pear")
str_detect(x, "e") # returns a logical vector of whether the string contains the letter "e"

words # built-in object containing common words
str_detect(words, "^t")
sum(str_detect(words, "^t")) # sum of the number of common words that begin with the letter "t"

# QUESTION: how many common words end in either w, x, y or z?
# TASK: calculate the proportion of common words that contain the letter "q". HINT: use another arithmetic function.

str_subset(words, "x$") # find the subset of words that end in "x"
str_count(words, "[aeiou]") # returns how many vowels are in each word
mean(str_count(words, "[aeiou]")) # returns how many vowels are in each word on average

# Let's combine what we've learned with what we know about data frames (tibbles in particular):
df <- tibble(
  word = words, 
  i = seq_along(word)
) # create a tibble with two columns: each word and a unique id for it
df

df %>% 
  filter(str_detect(word, "x$")) # find rows where the values of word end in "x"; this is equivalent to str_subset(words, "x$")

df %>% 
  mutate(
    vowels = str_count(word, "[aeiou]"),
    consonants = str_count(word, "[^aeiou]")
  )
# TASK: explore and describe the results of the above command.


# Looking for more string manipulation functions? Look no further than the `stringi` package`, 
# which has >200 functions. Have fun!

# Data Transformation #

# Wickham's dictum: Tidy datasets are all alike, but every messy dataset is messy in its own way.

# In this section we focus on the many tasks associated with wrangling or marshalling your data into a format suitable for analysis.
# In R-speak, this means constructing a "tidy" data set. This format will be familiar to you. A tidy data set is one that:
#	- is rectangular or tabular:
#		o every value has its own cell
#		o every variable has its own column
#		o every observation has its own row
# Social scientists are fluent in this format but as the known universe of data expands, corraling untidy data into a tidy format
# will be a necessary skill. Examples of untidy data sources include text corpora, information scraped from websites etc. However,
# even "recognisable" data sources can still be untidy.

# Tidying your data

# Let's look at some examples of representing data in multiple formats:
table1
table2
table3
# QUESTION: which data set is "tidy" and why?

# Now let's look at the tidyr package and how it can be used to address two common issues:
#	- one variable is spread across multiple columns
#	- one observation is spread across multiple rows

# Gathering

# The gather() function helps us address the first issue above. An example would be when there are multiple income variables
# e.g. inc2011, inc2012 etc. In this instance it would be better to "gather" these values as follows:
incdata <- tibble(pid = 1:3, inc2011 = c(10000, 25000, 98000), inc2012 = c(11000, 30000, 98000), 
	inc2013 = c(10000, 0, 100000), inc2014 = c(15000, 21000, 101000)) # create a tibble of sample data
incdata

inc_tidy <- incdata %>%
	gather(inc2011:inc2014, key = "year", value = "annual income")
inc_tidy
# gather() takes three parameters:
#	1. the variables you wish to gather the values of
#	2. the name of the variable that distinguishes values of the gathered variables (key)
#	3. the name of the variables that stores the values of the gathered variables (value)

# Spreading

# This is the opposite of gathering, and addresses the second issue above.
table2 # here the type column actually houses two variables: cases and population

tab2_tidy <- table2 %>%
	spread(key = type, value = count)
# spread() takes two parameters:
#	1. the name of the variable containing multiple variables (key)
#	3. the name of the variable that stores the values of the key variable (value)

# Perhaps you've already noticed what we're doing goes by another name: reshaping your data to long or wide format.
# That is, making our data narrower and longer, or wider and shorter.
# Having data in long format is necessary for conducting many types of longitudinal statistical modelling, so it is best to comfortable with 
# gathering and spreading ASAP.

# Missing values

# Think of missing values as being of two types:
#	1. Explicitly missing
#	2. Implicitly missing
# The former is the presence of an absence (i.e. missing values are flagged as NA or 99), the latter the absence of a presence (i.e.
# we do not possess an observation for that person for whatever reason).

# We can make implicit missing values explicit using the complete() function:
stocks <- tibble(
  year   = c(2015, 2015, 2015, 2015, 2016, 2016, 2016),
  qtr    = c(   1,    2,    3,    4,    2,    3,    4),
  return = c(1.88, 0.59, 0.35,   NA, 0.92, 0.17, 2.66)
) # create a sample data set

comp_stocks <- stocks %>% 
  complete(year, qtr)
# QUESTION: what effect has complete() had on the original tibble?

# Storing data as tibbles

library(nycflights13) # load in flights data
library(tidyverse) # load in dplyr via the tidyverse package

# A tibble (sounds like how an aristcrat pronounces "table") is a data frame that is very well suited to the tidyverse suite of functions
# It has a number of properties that makes it preferable to the use of base R data frames e.g. it doesn't convert strings to factors or change the names of variables.

# Creating tibbles

auto <- read_csv("C:/Users/mcdonndz-local/Desktop/github/aqmen_wp3/data/auto.csv")
str(auto)

as_tibble(auto)
str(auto)

# Viewing tibbles

auto 
# tibble automatically restricts the display of the data to ten rows and as many columns that fit the window. It also displays
# the variable type underneath the variable name (e.g. dbl, chr, date, int).

print(auto, n = Inf, width = Inf) # n controls the number of rows, and width the number of columns; 'Inf' tells print to display
# all rows and columns.
# Surpressing data display is a good default when working large data sets. However, you can override the default settings and
# tell RStudio to always print the entire tibble:
# options(tibble.print_min = Inf)
# options(tibble.width = Inf)

# Accessing tibble variables

auto$price # by name
auto[["price"]] # by name
auto[[1]] # by column position



# There are six core functions (or verbs) that form the essence of data transformation when using dplyr:
#	1. filter() -> pick certain observations or rows
#	2. select() -> pick certain variables or columns
#	3. mutate() -> create new variables
#	4. arrange() -> reorder observations or rows
#	5. summarise() -> produce summary statistics for a given set of variables
#	6. group_by() -> collapse observations or rows into groups (e.g. by ethicity, gender, local authority etc)

# All six work in the same manner:
#	1. take a data frame as input
#	2. require one or more variables in order to perform the function
# 	3. produce a new data frame as a result of executing the function

# 5.1.1 Filtering

filter(flights, month == 1, day == 1) # include observations for January 1st
# TASK: assign the result of this command to a new object and save as a csv file to the data_raw folder.

# Notice how including more than one argument is equivalent to saying "month == 1 AND day == 1". We could also specify the command
# as follows:
filter(flights, month == 1 & day == 1)
filter(flights, month == 1 | day == 1)
# QUESTION: how are the results of the second command different from the first? 

# 5.1.2 Selecting

select(flights, year, month, day) # select these three variables
select(flights, year:day) # select all variables between year and day (inclusive)
select(flights, -(year:day)) # select all variables except those from year to day (inclusive)

# There are a number of additional functions that help you refine your selection of variables:
select(flights, starts_with("y")) # select all variables that begin with the letter "y"
select(flights, contains("ay")) # select all variables that contain the characters "ay" (in that order)
# You can also employ regular expressions with select(); see the help documentation for more information.

# 5.1.3 Arranging

arrange(flights, year, month, day) # reorder observations by year, then by month, then by day (in ascending order)
arrange(flights, desc(year, month, day)) # reorder observations by year, then by month, then by day (in descending order)

# 5.1.4 Mutating

mutate(flights, gain = dep_delay - arr_delay,
  speed = distance / air_time * 60) # create two new variables in the flights data set

# 5.1.5 Summarising

summarise(flights, delay = mean(dep_delay, na.rm = TRUE)) # calculate the mean of dep_delay, ignoring missing values for ths value

# Summarise is much more useful when we combine it with group_by():
by_day <- group_by(flights, year, month, day)
summarise(by_day, delay = mean(dep_delay, na.rm = TRUE))
# TASK: describe the results produced by the above commands.

# Similar to select(), there are a number of additional summary functions that are worth knowing:
summarise(flights, av_delay = median(dep_delay), iqr_delay = iqr(dep_delay)) # median and interquartile range of departure delays
summarise(flights, airlines = n_distinct(carrier)) # number of unique airlines
# There are many more that we do not have the time to cover; see the help documentation for more information.

# 5.1.6 Grouping

# group_by() collapses observations into groups in order to produce summary statistics (e.g. mean age by ethnicity); it is best used in combination
# with other dplyr functions.

flights_sum <- group_by(flights, year, dest) %>%
	summarise(count = n())

flights_sml %>% 
  group_by(year, month, day) %>%
  filter(rank(desc(arr_delay)) < 10) # find the flights with the worst arrival delays

popular_dests <- flights %>% 
  group_by(dest) %>% 
  filter(n() > 365) # find the most popular destinations
popular_dests

# Piping #

# No, not the Scottish kind... Think of piping as a process for building summary tables. It takes
# data as an input, transfers it into some functions, and converts it into some results (like a pipeline). 
# In the words of Healy (2019):
# "A pipeline is typically a series of operations that do one or more of four things:
#   1. Group the data into the nested structure we want for our summary, such as “Religion by Region” or “Authors by Publications by Year”.
#   2. Filter or select pieces of the data by row, column, or both. This gets us the piece of the table we want to work on.
#   3. Mutate the data by creating new variables at the current level of grouping. This adds new columns to the table without aggregating it.
#   4. Summarize or aggregate the grouped data. This creates new variables at a higher level of grouping. For example we might calculate means with mean() or counts with n(). This results in a smaller, summary table, which we might do more things on if we want."
# The pipe operator = %>% (Ctrl + Shift + M)

vignette("dplyr") # explore some use cases employing the 'dplyr' commands

### REPLACE THIS EXAMPLE AND UPDATE DESCRIPTION ####
# Let's create a crosstab of religion by region:
rel_by_reg <- gss_sm %>% 
  group_by(bigregion, religion) %>%
  summarize(N = n()) %>%
  mutate(freq = N / sum(N), pct = round((freq*100), 0))

View(rel_by_reg) # display the summary table we created

# There's a lot going on in the above command, so let's take it piece-by-piece:
#   1. we create a new object
#   2. state which data set we are wanting to use
#   3. take that data and group it by region and religion (similar to the table() command earlier)
#   4. summarise this table by creating a new column (N) which is a count of observations
#   5. create two new variables: proportion and percentage of observations in each category

# If you were apprehensive about learning or using R, the above command is a valid reason:
# it is more cumbersome than performing the same task in Stata or SPSS, and requires us
# to explicitly delineate the logic and steps of producing the summary table.

# There are user-written packages that can produce crosstabs more efficiently than the above
# method (e.g. 'crosstab' from the 'descr' package) but using the pipe operator in this
# way is a very common task in R and you just have to get used to it :)




# Remove Non-numeric Values

# There are times when non-numeric characters are included in the values of a numeric variable
auto2 <- read_csv("C:/Users/mcdonndz-local/Desktop/github/aqmen_wp3/data/auto2.csv")
str(auto2$price) # stored as character data type

auto2$price <- as.numeric(gsub("[$,]", "", auto2$price)) # replace instances of "$" with a blank space ""
# TASK: check the class of the price variable - is it numeric or character?

# Subsetting Data

auto_sub <- subset(auto, select = c(make, price, foreign)) # keep three variables
auto_sub

auto_sub <- subset(auto, select = c(make:headroom)) # keep a range of variables
auto_sub
# You can combine the use of "," and ":" to select specific variables and a range in the one use of subset()

# Recoding Variables

# We will use the recode() function provided by the car package.

install.packages("car")
library(car)

# Renaming Variables


# Joining Data Sets #

# Sometimes the data we need for an analysis are stored in separate data sets: either we need additional variables for a set of
# observations, or additional observations for the same set of variables. This sounds a little abstract so let's look at examples
# using administrative data on the top 300 charities in Scotland (by income). This is an open data set provided by the Scottish
# Charity Regulator (OSCR): [https://www.oscr.org.uk/about-charities/search-the-register/the-300-highest-income-charities]

# Merging data

# The syntax (base R):
# merged_data <- merge(df1, df2, by="common id variable"), where df1 = data set #1, df2 = data set #2, "common id variable" = unique
# identifier of an observation. The data sets must be sorted by this id variable prior to merging.
# dplyr

chardata_1 <- read_csv("./data/scotchar_top300_details.csv")
chardata_1 <- arrange(chardata_1, `Charity Number`)
View(chardata_1) # contains organisational characteristics for top 300 charities

chardata_2 <- read_csv("./data/scotchar_top300_income.csv")
chardata_2 <- arrange(chardata_2, `Charity Number`)
View(chardata_2) # contains income information for top 300 charities

top300 <- merge(chardata_1, chardata_2, by = "`Charity Number`") # we only need Charity Number to merge but you can specify additional variables
str(top300)
names(top300) # now we have a single data set containing organisational and financial information for the charities

# Notice how we enclosed the variable "Charity Number" in backticks (i.e. "``"). This is because R (and other languages)
# struggle to parse variable names that contain spaces. There are a number of ways of dealing with this issue:
# 1. The way we have here.
# 2. Convert the data set to a tibble data frame using "tbl_df(chardata_1)".
# 3. Use the janitor package's clean_names() function.
# 4. Use the rename() function from dplyr to change the variable name(s).

# Appending data

# Note that variable names must be the same in across all data sets being appended.

mydata6080 = read_csv("http://www.princeton.edu/~otorres/mydata6080.csv") # sample data from 1960 to 1989
mydata9020 = read_csv("http://www.princeton.edu/~otorres/mydata9020.csv") # sample data from 1990 to 2013

View(mydata6080)
View(mydata9020)

mydata = rbind(mydata6080, mydata9020)
View(mydata)

# Reshaping Data #

# To long format

fredts_m <- fredts %>% select(date, sp500_i, monbase_i) %>%
    gather(key = series, value = score, sp500_i:monbase_i)


# Final Thoughts #

# Congratulations on progressing through the workshop. Our aim was to equip you with a proficiency in data wrangling using R as rapidly and painlessly as possible.
# While you have covered a great deal of material and skills, there is a bigger and badder world of R programming and wrangling out there.
# Here are some suggestions for where you might go next in your skills development:
#	- Wanting to work with big data? Collect, append and summarise 000s of data sets? Run simulations? It could be worth engaging in code
#		optimisation. See the Profiling R Code chapter in Peng (2015)
#
